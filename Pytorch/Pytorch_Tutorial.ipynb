{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Pytorch Tutorial",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/a0972340800/100-4/blob/master/Pytorch/Pytorch_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHILOGjOQbsQ"
      },
      "source": [
        "# **Pytorch Tutorial**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1zA7GupxdJv"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eqj90EkWbWx"
      },
      "source": [
        "**1. Pytorch Documentation Explanation with torch.max**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCXOg-iSQuk7"
      },
      "source": [
        "x = torch.randn(4,5)\n",
        "y = torch.randn(4,5)\n",
        "z = torch.randn(4,5)\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEqa9GFoWF78"
      },
      "source": [
        "# 1. max of entire tensor (torch.max(input) → Tensor)\n",
        "m = torch.max(x)\n",
        "print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wffThGDyWKxJ"
      },
      "source": [
        "# 2. max along a dimension (torch.max(input, dim, keepdim=False, *, out=None) → (Tensor, LongTensor))\n",
        "m, idx = torch.max(x,0)\n",
        "print(m)\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKDQW3tIXKg-"
      },
      "source": [
        "# 2-2\n",
        "m, idx = torch.max(input=x,dim=0)\n",
        "print(m)\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QZ6WRLyX3De"
      },
      "source": [
        "# 2-3\n",
        "m, idx = torch.max(x,0,False)\n",
        "print(m)\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqGuctkKbUEn"
      },
      "source": [
        "# 2-4\n",
        "m, idx = torch.max(x,dim=0,keepdim=True)\n",
        "print(m)\n",
        "print(idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OMzxuMlZPIu"
      },
      "source": [
        "# 2-5\n",
        "p = (m,idx)\n",
        "torch.max(x,0,False,out=p)\n",
        "print(p[0])\n",
        "print(p[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uhd4TqGTbD2c"
      },
      "source": [
        "# 2-6\n",
        "p = (m,idx)\n",
        "torch.max(x,0,False,p)\n",
        "print(p[0])\n",
        "print(p[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbxjUSOXxN0n"
      },
      "source": [
        "# 2-7\n",
        "m, idx = torch.max(x,True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMwhGLlGWYaR"
      },
      "source": [
        "# 3. max(choose max) operators on two tensors (torch.max(input, other, *, out=None) → Tensor)\n",
        "t = torch.max(x,y)\n",
        "print(t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFxRKu2Dedwb"
      },
      "source": [
        "**2. Common errors**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMcRyMxGwhul"
      },
      "source": [
        "The following code blocks show some common errors while using the torch library. First, execute the code with error, and then execute the next code block to fix the error. You need to change the runtime to GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eX-kKdi6ynFf"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-muJ4KKreoP2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "outputId": "c1d5c3a5-9540-4145-d80c-3cbca18a1deb"
      },
      "source": [
        "# 1. different device error\n",
        "model = torch.nn.Linear(5,1).to(\"cuda:0\")\n",
        "x = torch.Tensor([1,2,3,4,5]).to(\"cpu\")\n",
        "y = model(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-12e5b7d55705>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1692\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1693\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Tensor for 'out' is on CPU, Tensor for argument #1 'self' is on CPU, but expected them to be on GPU (while checking arguments for addmm)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a54PqxJLe9-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "909d3693-236f-4419-f269-8fb443ef7534"
      },
      "source": [
        "# 1. different device error (fixed)\n",
        "x = torch.Tensor([1,2,3,4,5]).to(\"cuda:0\")\n",
        "y = model(x)\n",
        "print(y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7OHtZwbi7Qw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "2a7d2dd0-6498-4da0-9591-3554c1739046"
      },
      "source": [
        "# 2. mismatched dimensions error\n",
        "x = torch.randn(4,5)\n",
        "y= torch.randn(5,4)\n",
        "z = x + y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7fa8b244df3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVynzvrskFCD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926dc01c-be6f-48e1-ad39-a5bcecebc513"
      },
      "source": [
        "# 2. mismatched dimensions error (fixed)\n",
        "y= y.transpose(0,1)\n",
        "z = x + y\n",
        "print(z.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([4, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgzgb9gJANod",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "21b58850-b3f1-4f2a-db5d-cc45e47ccbea"
      },
      "source": [
        "# 3. cuda out of memory error\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "resnet18 = models.resnet18().to(\"cuda:0\") # Neural Networks for Image Recognition\n",
        "data = torch.randn(2048,3,244,244) # Create fake data (512 images)\n",
        "out = resnet18(data.to(\"cuda:0\")) # Use Data as Input and Feed to Model\n",
        "print(out.shape)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-711923c7f347>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresnet18\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Neural Networks for Image Recognition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m244\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m244\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Create fake data (512 images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet18\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Use Data as Input and Feed to Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2056\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2057\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2058\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2059\u001b[0m     )\n\u001b[1;32m   2060\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 7.27 GiB (GPU 0; 14.76 GiB total capacity; 8.74 GiB already allocated; 4.42 GiB free; 9.42 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPksKnB_w343",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbee46ad-e63e-4bfc-8971-452895dd7a15"
      },
      "source": [
        "# 3. cuda out of memory error (fixed)\n",
        "for d in data:\n",
        "  out = resnet18(d.to(\"cuda:0\").unsqueeze(0))\n",
        "print(out.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 1000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqszlxEE0Bk0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "outputId": "a698b34d-00a8-4067-ddc5-180cb4c8eeaa"
      },
      "source": [
        "# 4. mismatched tensor type\n",
        "import torch.nn as nn\n",
        "L = nn.CrossEntropyLoss()\n",
        "outs = torch.randn(5,5)\n",
        "labels = torch.Tensor([1,2,3,4,0])\n",
        "lossval = L(outs,labels) # Calculate CrossEntropyLoss between outs and labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-60a5d1aad216>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlossval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Calculate CrossEntropyLoss between outs and labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    960\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 962\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2466\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2467\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2468\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2470\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2262\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2264\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2265\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZwgwup_1dgS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaf1de76-7ef2-4ca4-b87d-8482a3117249"
      },
      "source": [
        "# 4. mismatched tensor type (fixed)\n",
        "labels = labels.long()\n",
        "lossval = L(outs,labels)\n",
        "print(lossval)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(2.6215)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSuNdA8F06dK"
      },
      "source": [
        "**3. More on dataset and dataloader**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in84z_xu1rE6"
      },
      "source": [
        "A dataset is a cluster of data in a organized way. A dataloader is a loader which can iterate through the data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34zfh-c22Qqs"
      },
      "source": [
        "Let a dataset be the English alphabets \"abcdefghijklmnopqrstuvwxyz\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaiHofty1qKA"
      },
      "source": [
        "dataset = \"abcdefghijklmnopqrstuvwxyz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0jwhVa12h3a"
      },
      "source": [
        "A simple dataloader could be implemented with the python code \"for\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWC5Wwbv2egy"
      },
      "source": [
        "for datapoint in dataset:\n",
        "  print(datapoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n33VKzkG2y2U"
      },
      "source": [
        "When using the dataloader, we often like to shuffle the data. This is where torch.utils.data.DataLoader comes in handy. If each data is an index (0,1,2...) from the view of torch.utils.data.DataLoader, shuffling can simply be done by shuffling an index array.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MXUUKQ65APf"
      },
      "source": [
        "torch.utils.data.DataLoader will need two imformation to fulfill its role. First, it needs to know the length of the data. Second, once torch.utils.data.DataLoader outputs the index of the shuffling results, the dataset needs to return the corresponding data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV5txsjK5j4j"
      },
      "source": [
        "Therefore, torch.utils.data.Dataset provides the imformation by two functions, `__len__()` and `__getitem__()` to support torch.utils.data.Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0IEkemJ5ajD"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "class ExampleDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    self.data = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "  def __getitem__(self,idx): # if the index is idx, what will be the data?\n",
        "    return self.data[idx]\n",
        "\n",
        "  def __len__(self): # What is the length of the dataset\n",
        "    return len(self.data)\n",
        "\n",
        "dataset1 = ExampleDataset() # create the dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset = dataset1,shuffle = True,batch_size = 1)\n",
        "for datapoint in dataloader:\n",
        "  print(datapoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTt-ZTid9S2n"
      },
      "source": [
        "A simple data augmentation technique can be done by changing the code in `__len__()` and `__getitem__()`. Suppose we want to double the length of the dataset by adding in the uppercase letters, using only the lowercase dataset, you can change the dataset to the following."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Wn3BA2j-NXl"
      },
      "source": [
        "import torch.utils.data\n",
        "class ExampleDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self):\n",
        "    self.data = \"abcdefghijklmnopqrstuvwxyz\"\n",
        "\n",
        "  def __getitem__(self,idx): # if the index is idx, what will be the data?\n",
        "    if idx >= len(self.data): # if the index >= 26, return upper case letter\n",
        "      return self.data[idx%26].upper()\n",
        "    else: # if the index < 26, return lower case, return lower case letter\n",
        "      return self.data[idx]\n",
        "\n",
        "  def __len__(self): # What is the length of the dataset\n",
        "    return 2 * len(self.data) # The length is now twice as large\n",
        "\n",
        "dataset1 = ExampleDataset() # create the dataset\n",
        "dataloader = torch.utils.data.DataLoader(dataset = dataset1,shuffle = True,batch_size = 1)\n",
        "for datapoint in dataloader:\n",
        "  print(datapoint)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SED1wuoOdpBF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "egh42UR8dqyA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 1: COVID-19 Cases Prediction (Regression)"
      ],
      "metadata": {
        "id": "wbheyaK3dxz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download Data"
      ],
      "metadata": {
        "id": "Lug0lTrjd5SD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_path = 'covid.train.csv'  # path to training data\n",
        "# 這行程式碼將字串 'covid.train.csv' 賦值給變數 tr_path，表示訓練數據檔案的路徑或名稱。功能：用於記錄訓練檔案的檔案名稱，稍後可以使用這個變數來讀取或操作該檔案。\n",
        "tt_path = 'covid.test.csv'   # path to testing data\n",
        "# 這行程式碼將字串 'covid.test.csv' 賦值給變數 tt_path，表示測試數據檔案的路徑或名稱。功能：用於記錄測試檔案的檔案名稱，稍後可以使用這個變數來讀取或操作該檔案。\n",
        "!gdown --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF' --output covid.train.csv # 這行程式碼使用了 gdown 工具從 Google Drive 上下載檔案，並將其存儲為名為 covid.train.csv 的檔案。\n",
        "# !：在 Jupyter Notebook 或類似的互動式 Python 環境中，! 表示執行系統指令。這裡執行的是 gdown 命令。\n",
        "# gdown：這是一個 Python 外部的工具，用於通過 Google Drive 檔案 ID 下載檔案。\n",
        "# --id '19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF'：指定要下載的檔案的 Google Drive ID。這個 ID 是唯一的，用於識別 Google Drive 中的特定檔案。\n",
        "# --output covid.train.csv：指定下載後的檔案名稱為 covid.train.csv，並將其存儲到當前的工作目錄中。\n",
        "!gdown --id '1CE240jLm2npU-tdz81-oVKEF3T2yfT1O' --output covid.test.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hDFFmp3d9vu",
        "outputId": "0e38391e-bdda-4253-efe7-fdf7f463c5f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CCyCgJrUxtvgZF53vnctJiOJ23T5mqF\n",
            "To: /content/covid.train.csv\n",
            "100% 2.00M/2.00M [00:00<00:00, 207MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  warnings.warn(\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CE240jLm2npU-tdz81-oVKEF3T2yfT1O\n",
            "To: /content/covid.test.csv\n",
            "100% 651k/651k [00:00<00:00, 144MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Some Packages"
      ],
      "metadata": {
        "id": "asU2giKJfVN0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch\n",
        "import torch\n",
        "# torch:PyTorch 的核心庫，用於實現張量運算和深度學習模型的開發。\n",
        "import torch.nn as nn\n",
        "# torch.nn:PyTorch 的神經網絡模組，提供了各種神經網絡層和模型構建的工具。\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# torch.utils.data.Dataset:提供一個基礎類別，用於定義和加載自訂數據集。\n",
        "# torch.utils.data.DataLoader:用於將 Dataset 包裝成可迭代的數據加載器，支持批量處理、隨機打亂數據等功能。\n",
        "\n",
        "# For data preprocess\n",
        "import numpy as np\n",
        "# numpy:用於進行高效的數值運算，例如陣列操作。\n",
        "import csv\n",
        "# csv:用於處理 CSV 文件格式的數據。\n",
        "import os\n",
        "# os:提供操作系統功能，例如檔案和目錄操作。\n",
        "\n",
        "# For plotting\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.pyplot:用於資料視覺化，繪製圖表（如折線圖、散點圖等）。\n",
        "from matplotlib.pyplot import figure\n",
        "# figure:用於建立新的圖表畫布，方便控制圖的尺寸。\n",
        "\n",
        "myseed = 42069  # set a random seed for reproducibility\n",
        "# myseed = 42069:自訂的隨機種子值，可以選擇任何整數。隨機種子可以讓每次運行程式時的結果保持一樣。\n",
        "torch.backends.cudnn.deterministic = True\n",
        "# torch.backends.cudnn.deterministic = True:強制使用確定性的操作，保證相同輸入下產生相同的輸出。開啟後會略微降低執行效能。\n",
        "torch.backends.cudnn.benchmark = False\n",
        "# torch.backends.cudnn.benchmark = False:禁止動態調整最佳內核算法，這樣可以保證結果穩定。\n",
        "np.random.seed(myseed)\n",
        "# np.random.seed(myseed):設置 NumPy 的隨機數種子，確保數據處理過程的穩定性。\n",
        "torch.manual_seed(myseed)\n",
        "# torch.manual_seed(myseed):設置 PyTorch 的隨機數種子，用於 CPU 運算。\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(myseed)\n",
        "    # torch.cuda.manual_seed_all(myseed):如果 GPU 可用，為所有 GPU 設置隨機數種子。"
      ],
      "metadata": {
        "id": "oXcfN7WSfV_V"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some Utilities"
      ],
      "metadata": {
        "id": "morP-dHtiP4P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_device():\n",
        "    ''' Get device (if GPU is available, use GPU) '''\n",
        "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "# 這個函數用於檢查當前的硬體環境是否支持 GPU。如果支持，則返回 'cuda'（代表使用 GPU）；如果不支持，則返回 'cpu'（代表使用 CPU）\n",
        "# torch.cuda.is_available():PyTorch 提供的函數，用於檢測是否有可用的 GPU。如果有可用的 GPU 並且已正確安裝 CUDA，返回 True；否則返回 False。\n",
        "\n",
        "def plot_learning_curve(loss_record, title=''):\n",
        "  # 這個函數用於繪製神經網絡訓練過程中的學習曲線，包括訓練損失（train loss）和驗證損失（dev loss）。\n",
        "  # loss_record:一個字典，包含兩個鍵：'train': 訓練過程中每個步驟的損失值列表。'dev': 驗證過程中每個步驟的損失值列表。\n",
        "  # title:學習曲線的標題，可選，默認為空字串。\n",
        "    ''' Plot learning curve of your DNN (train & dev loss) '''\n",
        "    total_steps = len(loss_record['train'])\n",
        "    x_1 = range(total_steps)\n",
        "    # 訓練損失的橫軸對應的是總步驟數。\n",
        "    x_2 = x_1[::len(loss_record['train']) // len(loss_record['dev'])]\n",
        "    # 驗證損失通常記錄的頻率較低，這行程式碼將其橫軸按比例對齊到訓練損失。\n",
        "    # len(loss_record['train']):表示訓練損失記錄的總步數(總數據點數量)。\n",
        "    # len(loss_record['dev']):表示驗證損失記錄的總步數(總數據點數量)。驗證損失通常記錄的頻率低於訓練損失，因此這個數值通常小於 len(loss_record['train'])。\n",
        "    # len(loss_record['train']) // len(loss_record['dev']):計算訓練損失與驗證損失的比例，得到一個整數，表示每隔多少個訓練步驟記錄一次驗證損失。 A // B表示A除以B\n",
        "    # x_1[::步進值]:通過切片操作從 x_1 中按指定的間隔選取值。步進值 = len(loss_record['train']) // len(loss_record['dev'])，確保驗證損失的橫軸與訓練損失對齊。\n",
        "    figure(figsize=(6, 4))\n",
        "    plt.plot(x_1, loss_record['train'], c='tab:red', label='train')\n",
        "    # 訓練損失(紅色)\n",
        "    plt.plot(x_2, loss_record['dev'], c='tab:cyan', label='dev')\n",
        "    # 驗證損失(藍色)\n",
        "    plt.ylim(0.0, 5.)\n",
        "    plt.xlabel('Training steps')\n",
        "    plt.ylabel('MSE loss')\n",
        "    plt.title('Learning curve of {}'.format(title))\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # 圖表設定\n",
        "\n",
        "\n",
        "def plot_pred(dv_set, model, device, lim=35., preds=None, targets=None):\n",
        "  # plot_pred 函數：接收驗證集 dv_set、模型 model、計算設備 device，以及預測值和目標值(可選)。\n",
        "  # 如果未提供預測值和目標值，函數會利用模型和驗證集計算它們。\n",
        "  # 生成一個散點圖，其中每個點表示一個樣本的真實值和模型預測值。\n",
        "  # dv_set:驗證數據集(通常是 PyTorch 的 DataLoader)，包含樣本特徵和對應的真實目標值。\n",
        "  # model:預訓練好的深度神經網絡模型，用於計算預測值。\n",
        "  # device:設備類型（如 'cuda' 或 'cpu'），決定數據和模型運算的硬體。\n",
        "  # lim:圖表的最大範圍(上面設定 35)，用於設定 x 和 y 軸的範圍。\n",
        "  # preds 和 targets:預測值和真實值。如果已經計算好，可以直接傳入以繪圖；否則函數會計算它們。\n",
        "    ''' Plot prediction of your DNN '''\n",
        "    if preds is None or targets is None:\n",
        "    # 檢查是否已經提供了預測值 preds 和目標值 targets。\n",
        "    # 如果這兩者之一為空，表示需要從驗證數據集和模型中計算它們。\n",
        "        model.eval()\n",
        "        # model.eval()：進入評估模式，禁用 Dropout 等隨機操作，確保模型行為穩定。\n",
        "        preds, targets = [], []\n",
        "        # 用於存放模型對每批驗證數據的預測結果(preds)和真實值(targets)\n",
        "        for x, y in dv_set:\n",
        "        # dv_set 通常是 PyTorch 的 DataLoader，每次提供一批數據。\n",
        "        # x 是樣本的特徵，y 是對應的真實標籤(目標值)。\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            # 使用 to(device) 方法將數據移動到指定的計算設備上(例如 CPU 或 GPU)。\n",
        "            # 保證數據和模型在同一設備上，避免計算錯誤。\n",
        "            with torch.no_grad():\n",
        "            # torch.no_grad()：避免儲存中間結果的梯度，節省記憶體和計算資源。\n",
        "                pred = model(x)\n",
        "                # 將一批特徵 x 傳入模型，得到對應的預測值 pred。\n",
        "                preds.append(pred.detach().cpu())\n",
        "                # detach().cpu()：從計算圖中分離張量，移動到 CPU，方便後續處理。\n",
        "                # detach()：從計算圖中分離張量，避免影響後續操作。\n",
        "                # cpu()：將張量移動到 CPU，方便後續處理。\n",
        "                # 使用 append 方法將每批結果添加到列表中。\n",
        "                targets.append(y.detach().cpu())\n",
        "        preds = torch.cat(preds, dim=0).numpy()\n",
        "        targets = torch.cat(targets, dim=0).numpy()\n",
        "        # 將 preds 中的所有批次結果在第一個維度(樣本數量維度)上拼接，形成一個完整的張量。\n",
        "        # .numpy()：將 PyTorch 張量轉換為 NumPy 陣列，方便進一步操作或繪圖。\n",
        "\n",
        "    figure(figsize=(5, 5))\n",
        "    plt.scatter(targets, preds, c='r', alpha=0.5)\n",
        "    # 每個點的 x 座標是目標值(真實值 targets)，y 座標是模型的預測值(preds)。\n",
        "    # 點的顏色為紅色，透明度設為 0.5。\n",
        "    plt.plot([-0.2, lim], [-0.2, lim], c='b')\n",
        "    # 繪製一條藍色對角線，表示理想情況下預測值與目標值完全一致的參考線。\n",
        "    plt.xlim(-0.2, lim)\n",
        "    plt.ylim(-0.2, lim)\n",
        "    # 設定 x 和 y 軸範圍，方便比較。\n",
        "    plt.xlabel('ground truth value')\n",
        "    plt.ylabel('predicted value')\n",
        "    plt.title('Ground Truth v.s. Prediction')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "xN1QKu4QiQSy"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess\n",
        "\n",
        "We have three kinds of datasets:\\\n",
        "train: for training\\\n",
        "dev: for validation\\\n",
        "test: for testing (w/o target value)"
      ],
      "metadata": {
        "id": "gVEaIE9CnoCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The COVID19Dataset below does:\\\n",
        "read .csv files\\\n",
        "extract features\\\n",
        "split covid.train.csv into train/dev sets\\\n",
        "normalize features\\\n",
        "Finishing TODO below might make you pass medium baseline."
      ],
      "metadata": {
        "id": "Qp6ruidWoBet"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class COVID19Dataset(Dataset):\n",
        "    ''' Dataset for loading and preprocessing the COVID19 dataset '''\n",
        "    # COVID19Dataset 是一個繼承自 PyTorch Dataset 的類別，目的是實現資料加載和處理的自定義邏輯。\n",
        "    # 作用：將數據集從文件中讀取到內存。進行必要的數據預處理，方便後續模型使用。\n",
        "    def __init__(self,\n",
        "                 path, # 資料集文件的路徑，例如 covid.train.csv 或 covid.test.csv。\n",
        "                 mode='train', # 指定數據集模式（訓練或測試）。這可能影響數據處理方式，例如是否包含目標值(標籤)。\n",
        "                 target_only=False): # 用於選擇是否只使用部分特徵。\n",
        "                                     # 預設為 False，表示使用所有特徵。\n",
        "                                     # 當為 True 時，會根據特定邏輯(如代碼中的 TODO 部分)只選擇部分特徵。\n",
        "        self.mode = mode\n",
        "\n",
        "        # Read data into numpy arrays\n",
        "        with open(path, 'r') as fp:\n",
        "            data = list(csv.reader(fp))\n",
        "            # 將所有行存入列表 data 中，包含表頭和數據。\n",
        "            # csv.reader(fp)：讀取 CSV 文件的內容，每行作為一個列表。\n",
        "            data = np.array(data[1:])[:, 1:].astype(float)\n",
        "            # data[1:]：跳過第一行(表頭)。\n",
        "            # [:, 1:]：跳過第一列(通常是樣本 ID 或其他無關的識別符)。\n",
        "            # .astype(float)：將數據轉換為浮點數類型，方便後續數值運算。\n",
        "\n",
        "        if not target_only:\n",
        "        # 當 target_only 為 False 時，選擇所有特徵。\n",
        "            feats = list(range(93))\n",
        "            # 建立包含 0 到 92 的索引列表，表示選取數據的所有 93 個特徵。\n",
        "        else:\n",
        "        # 當 target_only 為 True 時，僅選擇特定特徵（未實現）。\n",
        "            # TODO: Using 40 states & 2 tested_positive features (indices = 57 & 75)\n",
        "            pass\n",
        "\n",
        "        if mode == 'test':\n",
        "            # Testing data\n",
        "            # data: 893 x 93 (40 states + day 1 (18) + day 2 (18) + day 3 (17))\n",
        "            data = data[:, feats]\n",
        "            # 使用 feats 選取特定的特徵列。此處的特徵列數可能是 93 列，包含 40 個狀態特徵與 3 天的特徵。\n",
        "            # feats：是一個包含列索引的列表或數組，這些列索引用於選擇需要的特徵。\n",
        "            # 例如，如果 feats = [0, 2, 5]，那麼 data[:, feats] 就會選擇原數據中的第 0 列、第 2 列和第 5 列作為新數據的特徵。\n",
        "            # :（冒號）：這是 numpy 中的切片操作符，表示選擇所有行。因此，data[:, feats] 表示選擇所有樣本的指定列。\n",
        "            self.data = torch.FloatTensor(data)\n",
        "            # 將 NumPy 陣列轉換為 PyTorch 的浮點張量。\n",
        "            # 結果：self.data 是一個 PyTorch 張量，用於後續的模型輸入。\n",
        "        else:\n",
        "            # Training data (train/dev sets)\n",
        "            # data: 2700 x 94 (40 states + day 1 (18) + day 2 (18) + day 3 (18))\n",
        "            target = data[:, -1]\n",
        "            # 提取目標值列，存入變數 target。\n",
        "            data = data[:, feats]\n",
        "            # 根據 feats 選取特定的特徵列。\n",
        "\n",
        "            # Splitting training data into train & dev sets\n",
        "            if mode == 'train':\n",
        "            # mode == 'train':這個條件判斷是用來確定當前處於訓練模式。如果 mode 的值為 'train'，則進行後續操作。\n",
        "                indices = [i for i in range(len(data)) if i % 10 != 0]\n",
        "                # range(len(data)) 會產生一個範圍，該範圍包含了數據集 data 中所有樣本的索引。假設 data 有 2700 筆樣本，這段程式碼會生成從 0 到 2699 的索引範圍。\n",
        "                # i % 10 != 0 是一個條件過濾表達式，表示選擇那些 i (索引值) 不是 10 的倍數的樣本。\n",
        "                # i % 10 是對索引 i 做除以 10 的餘數運算，如果結果不是 0，則 i 不是 10 的倍數，該樣本會被選中。\n",
        "                # 選擇 90% 的數據用於訓練。\n",
        "            elif mode == 'dev':\n",
        "                indices = [i for i in range(len(data)) if i % 10 == 0]\n",
        "                # 選擇 10% 的數據用於驗證。\n",
        "\n",
        "            # Convert data into PyTorch tensors\n",
        "            self.data = torch.FloatTensor(data[indices])\n",
        "            # torch.FloatTensor：將過濾後的數據和目標值轉換為 PyTorch 浮點張量，便於後續用於模型訓練或評估。\n",
        "            # data[indices]：根據選定的索引過濾數據，僅保留訓練或驗證所需的樣本。\n",
        "            self.target = torch.FloatTensor(target[indices])\n",
        "\n",
        "        # Normalize features (you may remove this part to see what will happen)\n",
        "        self.data[:, 40:] = \\\n",
        "        # self.data 是一個包含所有特徵的張量。這一行程式碼選擇了 self.data 中第 40 列及之後的所有列（即特徵），進行標準化處理。\n",
        "            (self.data[:, 40:] - self.data[:, 40:].mean(dim=0, keepdim=True)) \\\n",
        "            # 這一步是將每列的數據減去該列的均值，這樣每列的均值變為 0。\n",
        "            / self.data[:, 40:].std(dim=0, keepdim=True)\n",
        "            # 然後用每列的標準差對每個數據點進行除法，這樣每列的標準差變為 1。\n",
        "            # dim=0：這個參數表示沿著列(第 0 維)計算均值和標準差，即每列的均值和標準差。\n",
        "            # keepdim=True：這個參數保證了輸出的均值和標準差維度不會被簡化，保持與輸入數據相同的維度。\n",
        "\n",
        "        self.dim = self.data.shape[1]\n",
        "        # self.data.shape[1] 返回 self.data 的列數（即每個樣本的特徵數量），並將其賦值給 self.dim。\n",
        "        # self.dim 用來記錄數據集中特徵的數量。\n",
        "\n",
        "        print('Finished reading the {} set of COVID19 Dataset ({} samples found, each dim = {})'\n",
        "              .format(mode, len(self.data), self.dim))\n",
        "        # mode：這是數據集的模式（'train'、'dev' 或 'test'），表示當前載入的是訓練集、驗證集還是測試集。\n",
        "        # len(self.data)：len(self.data) 返回數據集中樣本的數量。\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "    # __getitem__ 是 Python 中的一個特殊方法，它允許對象像列表一樣進行索引。當我們使用 dataset[index] 訪問數據集中的某個樣本時，實際上會調用這個方法。\n",
        "        # Returns one sample at a time\n",
        "        if self.mode in ['train', 'dev']:\n",
        "        # self.mode 是一個標誌，表示當前的數據集處於哪個模式，可能的值有 'train'(訓練集)、'dev'(驗證集)或 'test'(測試集)。\n",
        "            # For training\n",
        "            return self.data[index], self.target[index]\n",
        "        else:\n",
        "            # For testing (no target)\n",
        "            return self.data[index]\n",
        "\n",
        "    def __len__(self):\n",
        "    # __len__ 是 Python 中的特殊方法，用來返回對象的長度。當我們對一個數據集（例如 dataset）調用 len(dataset) 時，Python 會自動調用這個方法。\n",
        "    # 在 PyTorch 中，Dataset 類別需要實現這個方法來告訴 DataLoader 這個數據集有多少個樣本。\n",
        "        # Returns the size of the dataset\n",
        "        return len(self.data)\n",
        "        # self.data 是在 COVID19Dataset 類別中加載並處理過的特徵數據。\n",
        "        # self.data 是一個包含數據的 torch.Tensor，表示每個樣本的特徵。\n",
        "        # len(self.data) 返回 self.data 張量中的樣本數量(即數據集的大小)。\n",
        "        # 如果 self.data 中有 2700 個樣本，則 len(self.data) 返回 2700。"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "IiES-6tIoI1D",
        "outputId": "7eb9851b-6e12-4f05-9039-af86489adb54"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-7-c9d070cbf66a>, line 71)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-c9d070cbf66a>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    # self.data 是一個包含所有特徵的張量。這一行程式碼選擇了 self.data 中第 40 列及之後的所有列（即特徵），進行標準化處理。\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DataLoader\n",
        "\n",
        "A DataLoader loads data from a given Dataset into batches."
      ],
      "metadata": {
        "id": "eTzkp9l1IAw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_dataloader(path, mode, batch_size, n_jobs=0, target_only=False):\n",
        "# path：指定數據集文件的路徑(如 CSV 文件)，用來初始化 COVID19Dataset。\n",
        "# mode：指定數據集的模式，可以是 'train'(訓練集)、'dev'(驗證集)或 'test'(測試集)。這個參數會影響數據集的處理方式。\n",
        "# batch_size：指定每個批次的樣本數量。DataLoader 會按照這個數量將數據分批。\n",
        "# n_jobs（默認值為 0）：指定加載數據時使用的工作進程數量。n_jobs=0 表示不使用多進程。\n",
        "# target_only（默認值為 False）：指定是否只加載目標值。如果設為 True，則只返回目標值；如果設為 False，則返回特徵和目標值。\n",
        "    ''' Generates a dataset, then is put into a dataloader. '''\n",
        "    dataset = COVID19Dataset(path, mode=mode, target_only=target_only)  # Construct dataset\n",
        "    # 使用傳入的 path、mode 和 target_only 參數創建 COVID19Dataset 實例。COVID19Dataset 是自定義的數據集類別，它會讀取並處理數據。\n",
        "    # path：數據集的路徑。\n",
        "    # mode：設置數據集的模式（訓練、驗證或測試）。\n",
        "    # target_only：決定是否只加載目標值。\n",
        "    dataloader = DataLoader(\n",
        "    # DataLoader 是 PyTorch 提供的數據加載器，用於將數據集分成批次並有效地加載數據。\n",
        "        dataset, batch_size, # dataset：將創建的 COVID19Dataset 作為數據集。\n",
        "                             # batch_size：設置每個批次的樣本數量。\n",
        "        shuffle=(mode == 'train'), drop_last=False, # shuffle=(mode == 'train')：當 mode 是 'train' 時，設置為 True，表示在每次迭代時打亂數據；如果是驗證或測試模式，則不打亂數據（shuffle=False）。\n",
        "                                                    # drop_last=False：這表示如果批次中的樣本數量不足以填滿最後一個批次，則仍然包含這個批次。如果設置為 True，則丟棄最後一個不完整的批次。\n",
        "        num_workers=n_jobs, pin_memory=True)                            # Construct dataloader\n",
        "        # num_workers=n_jobs：指定用於數據加載的工作進程數量。n_jobs=0 表示使用單進程加載數據。\n",
        "        # pin_memory=True：設置為 True，這樣會將數據加載到鎖頁內存中，這樣可以加速將數據從 CPU 複製到 GPU 的過程，對於 GPU 加速訓練有幫助。\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "x3s0wQMtICnT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Network\n",
        "\n",
        "NeuralNet is an nn.Module designed for regression. The DNN consists of 2 fully-connected layers with ReLU activation. This module also included a function cal_loss for calculating loss."
      ],
      "metadata": {
        "id": "yFPGnSZxxqFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNet(nn.Module):\n",
        "# NeuralNet 是一個簡單的全連接神經網絡。\n",
        "# 繼承自 nn.Module，使得這個類別可以使用 PyTorch 提供的各種工具（如自動微分、權重管理等）。\n",
        "    ''' A simple fully-connected deep neural network '''\n",
        "    def __init__(self, input_dim):\n",
        "    # 定義了神經網絡的結構，接收輸入特徵數量 input_dim 作為參數。\n",
        "        super(NeuralNet, self).__init__()\n",
        "        # 初始化父類（nn.Module），使得 PyTorch 能正確地追蹤這個模型中的參數。\n",
        "\n",
        "        # Define your neural network here\n",
        "        # TODO: How to modify this model to achieve better performance?\n",
        "        self.net = nn.Sequential( # nn.Sequential：將多層操作按順序組合起來，簡化了模型的定義。\n",
        "            nn.Linear(input_dim, 64), # 全連接層，nn.Linear(in_features, out_features)，將輸入的維度映射到指定的輸出維度。\n",
        "                                      # 第一層：輸入維度 input_dim，輸出維度 64。\n",
        "            nn.ReLU(), # 激活函數，將輸入的負值置為 0，保留正值，具有良好的梯度傳播效果。\n",
        "            nn.Linear(64, 1) # 第二層：輸入維度 64，輸出維度 1。\n",
        "        )\n",
        "\n",
        "        # Mean squared error loss\n",
        "        self.criterion = nn.MSELoss(reduction='mean')\n",
        "        # nn.MSELoss：均方誤差損失函數，用於回歸任務。\n",
        "        # reduction='mean'：計算損失時，對所有樣本的損失值取平均。\n",
        "\n",
        "    def forward(self, x):\n",
        "    # forward(self, x)： 這是神經網路中的前向傳播方法，用於定義輸入數據如何通過模型進行計算以產生輸出。該方法通常實現在繼承自 torch.nn.Module 的自定義類別中。\n",
        "    # x：輸入的張量 (tensor)，形狀為 (batch_size, input_dim)，其中 batch_size 是批量大小，input_dim 是輸入的特徵數。\n",
        "        ''' Given input of size (batch_size x input_dim), compute output of the network '''\n",
        "        return self.net(x).squeeze(1)\n",
        "        # self.net(x)：將輸入 x 傳遞給網路 self.net。self.net 是在類別中定義的子網路，可能由多個層組成（例如線性層或激活函數）。\n",
        "        # squeeze(1)：移除輸出張量中維度大小為 1 的第二個維度。例如：如果輸出形狀為 (batch_size, 1)，則經過 .squeeze(1) 後，形狀會變成 (batch_size,)。\n",
        "\n",
        "    def cal_loss(self, pred, target):\n",
        "    # 這是用於計算損失的函式，根據模型的預測值 (pred) 和目標值 (target) 計算損失值。\n",
        "    # pred：模型的預測輸出，通常是張量(tensor)。\n",
        "    # target：真實標籤或目標值，形狀與 pred 通常相同。\n",
        "\n",
        "        ''' Calculate loss '''\n",
        "        # TODO: you may implement L1/L2 regularization here\n",
        "        return self.criterion(pred, target)\n",
        "        # self.criterion(pred, target)：\n",
        "        # 使用 self.criterion 計算損失，self.criterion 通常是 PyTorch 中定義的損失函式，例如：\n",
        "        # MSELoss（均方誤差，適用於回歸問題）。\n",
        "        # CrossEntropyLoss（交叉熵損失，適用於分類問題）。\n",
        "        # BCELoss（二元交叉熵損失，適用於二元分類）。"
      ],
      "metadata": {
        "id": "kMouwhQ6xtSG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train/Dev/Test"
      ],
      "metadata": {
        "id": "N8UtPJc_0BT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(tr_set, dv_set, model, config, device):\n",
        "# 這是用於訓練深度神經網路 (Deep Neural Network, DNN) 的函式。\n",
        "# tr_set: 訓練數據集。\n",
        "# dv_set: 驗證數據集（用於驗證模型表現，非訓練用）。\n",
        "# model: 欲訓練的深度學習模型。\n",
        "# config: 包含訓練相關超參數（如學習率、優化器類型等）的設定。\n",
        "# device: 設定模型運行的設備（如 cpu 或 cuda）。\n",
        "    ''' DNN training '''\n",
        "\n",
        "    n_epochs = config['n_epochs']  # Maximum number of epochs\n",
        "    # 從 config 中提取最大訓練輪數，確保訓練過程有一個終止條件。\n",
        "\n",
        "    # Setup optimizer\n",
        "    optimizer = getattr(torch.optim, config['optimizer'])(\n",
        "        model.parameters(), **config['optim_hparas'])\n",
        "\n",
        "    min_mse = 1000.\n",
        "    loss_record = {'train': [], 'dev': []}      # for recording training loss\n",
        "    early_stop_cnt = 0\n",
        "    epoch = 0\n",
        "    while epoch < n_epochs:\n",
        "        model.train()                           # set model to training mode\n",
        "        for x, y in tr_set:                     # iterate through the dataloader\n",
        "            optimizer.zero_grad()               # set gradient to zero\n",
        "            x, y = x.to(device), y.to(device)   # move data to device (cpu/cuda)\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "            mse_loss.backward()                 # compute gradient (backpropagation)\n",
        "            optimizer.step()                    # update model with optimizer\n",
        "            loss_record['train'].append(mse_loss.detach().cpu().item())\n",
        "\n",
        "        # After each epoch, test your model on the validation (development) set.\n",
        "        dev_mse = dev(dv_set, model, device)\n",
        "        if dev_mse < min_mse:\n",
        "            # Save model if your model improved\n",
        "            min_mse = dev_mse\n",
        "            print('Saving model (epoch = {:4d}, loss = {:.4f})'\n",
        "                .format(epoch + 1, min_mse))\n",
        "            torch.save(model.state_dict(), config['save_path'])  # Save model to specified path\n",
        "            early_stop_cnt = 0\n",
        "        else:\n",
        "            early_stop_cnt += 1\n",
        "\n",
        "        epoch += 1\n",
        "        loss_record['dev'].append(dev_mse)\n",
        "        if early_stop_cnt > config['early_stop']:\n",
        "            # Stop training if your model stops improving for \"config['early_stop']\" epochs.\n",
        "            break\n",
        "\n",
        "    print('Finished training after {} epochs'.format(epoch))\n",
        "    return min_mse, loss_record"
      ],
      "metadata": {
        "id": "lsfQU4xA0CvF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "J2RPClyM00R5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dev(dv_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    total_loss = 0\n",
        "    for x, y in dv_set:                         # iterate through the dataloader\n",
        "        x, y = x.to(device), y.to(device)       # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            mse_loss = model.cal_loss(pred, y)  # compute loss\n",
        "        total_loss += mse_loss.detach().cpu().item() * len(x)  # accumulate loss\n",
        "    total_loss = total_loss / len(dv_set.dataset)              # compute averaged loss\n",
        "\n",
        "    return total_loss"
      ],
      "metadata": {
        "id": "oHe-o18Z027g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "Da3xDcmn05oX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(tt_set, model, device):\n",
        "    model.eval()                                # set model to evalutation mode\n",
        "    preds = []\n",
        "    for x in tt_set:                            # iterate through the dataloader\n",
        "        x = x.to(device)                        # move data to device (cpu/cuda)\n",
        "        with torch.no_grad():                   # disable gradient calculation\n",
        "            pred = model(x)                     # forward pass (compute output)\n",
        "            preds.append(pred.detach().cpu())   # collect prediction\n",
        "    preds = torch.cat(preds, dim=0).numpy()     # concatenate all predictions and convert to a numpy array\n",
        "    return preds"
      ],
      "metadata": {
        "id": "Zf_E75TM08Ga"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Hyper-parameters\n",
        "\n",
        "config contains hyper-parameters for training and the path to save your model."
      ],
      "metadata": {
        "id": "J32CKmU70-HH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = get_device()                 # get the current available device ('cpu' or 'cuda')\n",
        "os.makedirs('models', exist_ok=True)  # The trained model will be saved to ./models/\n",
        "target_only = False                   # TODO: Using 40 states & 2 tested_positive features\n",
        "\n",
        "# TODO: How to tune these hyper-parameters to improve your model's performance?\n",
        "config = {\n",
        "    'n_epochs': 3000,                # maximum number of epochs\n",
        "    'batch_size': 270,               # mini-batch size for dataloader\n",
        "    'optimizer': 'SGD',              # optimization algorithm (optimizer in torch.optim)\n",
        "    'optim_hparas': {                # hyper-parameters for the optimizer (depends on which optimizer you are using)\n",
        "        'lr': 0.001,                 # learning rate of SGD\n",
        "        'momentum': 0.9              # momentum for SGD\n",
        "    },\n",
        "    'early_stop': 200,               # early stopping epochs (the number epochs since your model's last improvement)\n",
        "    'save_path': 'models/model.pth'  # your model will be saved here\n",
        "}"
      ],
      "metadata": {
        "id": "ofKhxYwt1BV1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data and model"
      ],
      "metadata": {
        "id": "Z7E43DDN1ENc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr_set = prep_dataloader(tr_path, 'train', config['batch_size'], target_only=target_only)\n",
        "dv_set = prep_dataloader(tr_path, 'dev', config['batch_size'], target_only=target_only)\n",
        "tt_set = prep_dataloader(tt_path, 'test', config['batch_size'], target_only=target_only)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "id": "-7bCQhTo1G3g",
        "outputId": "4a349af9-13d3-40e3-bcfe-cb74b79a0773"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'COVID19Dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a81455a69d6f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtr_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdv_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'dev'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtt_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprep_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtt_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-93dedda499bd>\u001b[0m in \u001b[0;36mprep_dataloader\u001b[0;34m(path, mode, batch_size, n_jobs, target_only)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# target_only（默認值為 False）：指定是否只加載目標值。如果設為 True，則只返回目標值；如果設為 False，則返回特徵和目標值。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m''' Generates a dataset, then is put into a dataloader. '''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOVID19Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_only\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Construct dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# 使用傳入的 path、mode 和 target_only 參數創建 COVID19Dataset 實例。COVID19Dataset 是自定義的數據集類別，它會讀取並處理數據。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# path：數據集的路徑。\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'COVID19Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Training!"
      ],
      "metadata": {
        "id": "VXtOySbm1If6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_loss, model_loss_record = train(tr_set, dv_set, model, config, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "raGeEVVG1KoY",
        "outputId": "521936e8-a6f7-4979-c86e-543793e30ff8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tr_set' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-07c5e7cc301a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_loss_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdv_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tr_set' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n",
        "\n",
        "The predictions of your model on testing set will be stored at pred.csv."
      ],
      "metadata": {
        "id": "qjx3Z66w1Mow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pred(preds, file):\n",
        "    ''' Save predictions to specified file '''\n",
        "    print('Saving results to {}'.format(file))\n",
        "    with open(file, 'w') as fp:\n",
        "        writer = csv.writer(fp)\n",
        "        writer.writerow(['id', 'tested_positive'])\n",
        "        for i, p in enumerate(preds):\n",
        "            writer.writerow([i, p])\n",
        "\n",
        "preds = test(tt_set, model, device)  # predict COVID-19 cases with your model\n",
        "save_pred(preds, 'pred.csv')         # save prediction file to pred.csv"
      ],
      "metadata": {
        "id": "HxzWqx5q1PsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hints\n",
        "\n",
        "# Simple Baseline\n",
        "\n",
        "Run sample code\n",
        "# Medium Baseline\n",
        "\n",
        "Feature selection: 40 states + 2 tested_positive (TODO in dataset)\n",
        "# Strong Baseline\n",
        "\n",
        "Feature selection (what other features are useful?)\\\n",
        "DNN architecture (layers? dimension? activation function?)\\\n",
        "Training (mini-batch? optimizer? learning rate?)\\\n",
        "L2 regularization\\\n",
        "There are some mistakes in the sample code, can you find them?"
      ],
      "metadata": {
        "id": "34oP8wa41SCm"
      }
    }
  ]
}